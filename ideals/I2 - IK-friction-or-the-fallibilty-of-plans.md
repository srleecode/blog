---
title: 'Friction or the fallibility of plans'
description: Friction
tags: Productivity
publishedDate: 11/4/2020
published: false
slugs:
    - ___UNPUBLISHED___kdqqtgiw_nYftHfsn7IJRnldxjAoaNeEsft7KZ2h2

---
# Friction or the fallibility of plans

## Perception, prediction and planning are all underpinned by models

We tend to think of perception, or more broadly seeing the world, as descriptive in nature, but it is actually more constructive. We create the world that we see because we cannot help but see it through ourselves, through the stories we weave from the contents of sensory experience, memories, motives and emotions.

In constructing that world that we see, the brain has two major problems: it doesn't have access to all the data and the data is does have access to is too much to process in a short amount of time. Therefore, in order to minimize cognitive load and build more complete representations quickly, the brain draws upon itself and in doing so is able to fill in and assume.

The world that we see is made up of internal, simplified and cognitively acceptable representations which will be referred to as mental models. As societies progress, so to does their ability to externalize and build repositories of these mental models which are externalized or expressed through artifacts like pictures, textual descriptions, art and mathematical formulas. These artifacts will be referred to as conceptual models.

## Models are fundamentally limited and exhibit increasing friction overtime

In general, the main reason that models are useful is that they have predictive power. That is, they allow us to understand and anticipate the behavior of the systems they represent. This predictive power is a measure of how closely a model’s predictions match the facts. Models, by their very definition, are simplified representations. Therefore, their predictive power is inherently limited. Models leave things out and they have to if they are to be usable. This means that the predictions derived from these models are always at some level limited in terms of their accuracy and the longevity of that accuracy.

> A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which accounts for its usefulness — Alfred Korzybski, Science and Sanity, p. 58.

Models are fundamentally limited and the world changes, therefore, as time progresses they become increasingly out of sync with that which they are modeling. This creates a mismatch between the model and what is being modeled and this mismatch manifests itself as a kind of friction¹ or difficulty when you try to use that model. Put another way, the mismatch causes the predictions from the model to become increasingly inaccurate and unhelpful and consequently actions derived from that model become increasingly unhelpful and inappropriate as well. A model with high friction is one in which the predictions derived from the model are likely to be inaccurate. Hence, higher friction equates to higher difficulty in using the model as a basis to take action or more broadly to make accurate predictions.

Friction arises due to many reasons. For example:

- Changes in the system being modelled - this causes an increase in friction unless the change is already accounted² for in the model. There are two ways to minimize this type of friction. The first is by creating high resolution (complicated) models and the second is to model only that which is constant. Both of these approaches have negative consequences and when taken to the extreme produce either unusable or unhelpful models. Higher resolution models lead to greater accuracy, but also reduce the usability of the model. Modeling only that which is constant, on the other hand, leads to greater longevity in the accuracy of the predictions, but places harsh restrictions on what can be modeled.
- Structural properties of the system being modeled that preclude or make predictability and accurate modeling difficult - not only is our ability to model things limited our ability to extract data from the world is also limited. A system with chaotic aspects requires a super precise and accurate model. A system with complex aspects requires not only a model of the system being modeled, but also models of a whole lot of other related systems and all their interactions. More chaos or complexity means that modelling will be more difficult.
- Informational uncertainties and unforeseeable differences stemming from the spatial-temporal dispersion of information – the often unavoidable dispersion of information in both space and time means that there is some information that will be for all practical purposes inaccessible. For example, much of the brain’s information processing is both dispersed and inaccessible to consciousness in any direct or real-time manner. If you don't have access to information, then it cannot be accounted for in the model and the model will have some level of friction because of this.
- A human centric modeling process necessarily includes some kinds of friction (biases) - the reason the brain creates models is so that it can anticipate and generate expectations. Our brains are for "producing future" (as the poet Valery once put it). This future has to be created both swiftly and reliably and the result has to be usable. These two needs for swiftness and usability have been present as the brain has evolved and so have molded it to allow, seek out or not consider certain kinds of friction in its models³.
- Limitations in the users, creators and manipulators of the model – since our models are created by and for humans they are impacted by the frailties and limitations of humanity, e.g. bounded rationality. The closer to their limits someone is the greater the impact on these capabilities. For example, danger and physical exertion can degrade orientation, precipitate poor decisions and produce slow, ragged, or even flawed execution of actions. There are finite limits, grounded in biology and evolution, in regards to the capabilities of human beings to:
  - receive sensory data
  - orient themselves, i.e. create an appropriate mental model, by integrating input with prior experience and knowledge
  - reach plausible decisions about what to do next
  - act upon those decisions

---

¹

The term 'friction' as used here is based on the Clausewitzian term that refers to the multitude of factors that distinguish real war from war on paper or our models from what actually occurs. Carl von Clausewitz famously wrote:

> “Everything in war is very simple, but the simplest thing is difficult,”

> “Action in war is like movement in a resistive element.”

Friction is the “effect of reality on ideas and intentions in war” — that is, the difference between plans and reality.

In this context, a broader meaning of ‘friction’ has been used. It has been used to refer to the level of how in-sync or out-of-sync a model is with what it is actually modeling. A model that has high levels of friction is one that is highly out-of-sync with what is modeling and a model with low friction is the opposite.

The word friction is used because as models become out-of-sync they become harder to use. We use models because they provide some benefit in predicting or understanding how the modeled system will behave. Models never start fully in-sync with what they are modeling, but should be somewhat in-sync if they are to be useful. This is because being in-sync is what allows the predictions derived from the model to be accurate and hence useful. A model with high friction is one in which the predictions derived from the model are likely to be inaccurate. Hence, higher friction equates to higher difficulty in using the model as a basis to take action or more broadly to make accurate predictions.

Clausewitzian friction is unavoidable. In the context of war, (Watts 2004) argues that ‘friction’ arises from structural aspects of combat interactions so deeply and irretrievably embedded in violent interactions between humans-in-the-loop systems that technological advances can never eliminate it, although they can certainly alter its manifestations. The three below factors are claimed to lead to friction in war:

- Constraints imposed by human physical and cognitive limits, whose magnitude and effects are inevitably magnified by the intense stresses, pressures and responses of actual combat
- Informational uncertainties and unforeseeable differences between perceived and actual reality stemming, ultimately, from the spatial temporal dispersion of information in the external environment, in friendly and enemy military organizations, and in the mental constructs of individual participants on both sides
- The structural nonlinearity of combat processes that can give rise to the long-term unpredictability of results and emergent phenomena by magnifying the effects of unknowable small differences and unforeseen events (or conversely, producing negligible results from large differences in inputs).

In Destruction & Creation (Boyd, 1979), Boyd makes the claim that concepts (mental constructs or models) and reality have a fundamentally divergent nature. That is, that their match-up is naturally going to decrease over time. Boyd also makes the claim that any inward-oriented effort to improve the match-up of concept (model) with observed reality will only increase the degree of mismatch. Boyd infers this from the following theories:

- Gödel's Incompleteness Theorem which tells us that in order to determine the consistency of any new system we must construct or uncover another system beyond it. Boyd draws from this that one must orient outside of one's present mental model in order to achieve an enlightened perspective of reality. We must see things anew or from a different perspective. This is because we cannot determine the consistency of the ‘system’ (in this context, the concept of reality and its matchup with observed reality) within itself because observations are used to shape, sharpen and formulate concepts. Concepts are then used in turn to shape and sharpen observations. This interrelationship means that they both must be incomplete:
  - A concept must be incomplete since we depend upon an ever-changing array of observations to shape or formulate it.
  - Our observations of reality must be incomplete since we depend upon a changing concept to shape or formulate the nature of new inquiries and observations.
- Heisenberg's Uncertainty Principle which tells us that there is a limit on our ability to observe reality with precision. This is because the act of observation heavily shapes reality. It is a form of interaction. Hence, an unavoidable consequence of precision in observation is greater influence or intrusion on the observed phenomena and the degree of intrusion by the observer affects the degree of confusion and disorder perceived by that observer. Boyd draws from this that any inward-oriented effort to improve the match-up of concept with observed reality will only increase the degree of mismatch as the more precisely we know the measured value of one quantity, the greater the uncertainty in another “conjugate” quantity.
- Second Law of Thermodynamics: which tells us that the entropy of any closed system always tends to increase, and thus the nature of any given system is continuously changing even as efforts are directed toward maintaining it in its original form. Boyd draws from this that whenever we attempt to do work or take action inside such a ’system’ (in this context, the concept of reality and its matchup with observed reality) we should anticipate an increase in entropy, i.e. an increase in confusion and disorder. Naturally, this means that we cannot determine the character or nature (consistency) of such a system within itself, since the system is moving irreversible toward a higher, yet unknown, state of confusion and disorder.

Boyd uses these theories to argue that the uncertainty and the related disorder associated with a closed-system can only be overcome by creating higher and broader more general concepts to represent reality through the dialectic cycle of destruction and creation. The dialectic cycle refers to Boyd’s hypothesis that there are only two ways to manipulate mental concepts to represent observed reality. We can start from a comprehensive whole and break it down to its particulars [general to specific also known as deduction, analysis, and differentiation] or we can start with the particulars and build towards a comprehensive whole [specific to general also known as induction, synthesis, and integration].

²
Friction not only arises in models. It is an inherent part of them. Our models are purposeful representations. They are created to help us interact with or to understand systems and so the information in them is constrained by how much it inhibits the usefulness or usability of the model. That is, there must be at some point a fundamental trade-off between the accuracy and usability of a model. This is known as Bonini's paradox:

- As a model of a complex system becomes more complete, it becomes less understandable
- As a model grows more realistic, it also becomes just as difficult to understand as the real-world processes it represents

Bonini's paradox tells us that finding the right model is really a matter of satisficing as our models only need to be minimally accurate. In fact, more accuracy can be detrimental as it leads to the model becoming more complicated, complex and unwieldy.

The trade-off between the accuracy and usability of a model means that there will often end be a need to use many different models as we work with things at different scales or with different requirements for accuracy. For example, we might at first use Newton’s laws of motion or "common sense" reasoning to describe everyday objects at everyday speeds, but these methods will eventually start giving us wrong results when you start looking at smaller and smaller or faster and faster things.

Models are also often only representations of the system at a certain point in time. To track changes that occur in the modeled system the models must include predictions or be subsumed into a new model that take into account those changes. So, to keep a model accurate you either need a complicated model or one that is frequently updated based on feedback.

³
Cognitive biases can arise for at least three reasons.

- Selection may favor useful short-cuts that tend to work in most circumstances, though they fall short of some normative standards (heuristics). Perhaps the most commonly invoked explanation for bias is as a necessary by-product of processing limitations—because information processing time and ability are limited, humans must use shortcuts or rules-of-thumb that are prone to break-down in systematic ways.
- The task at hand is not one for which the mind is designed (artifacts). Humans have evolved problem-solving mechanisms tailored to problems recurrently present over evolutionary history. When problems are framed in ways congruent with these adaptive problems (such as social contract violation), humans can be shown to use appropriate reasoning strategies. One criticism of classic heuristics and biases research is that the problems presented in the laboratory are not those for which the human brain has evolved and so we should not be surprised that people’s responses to them appear to be systematically irrational. For example, in human statistical prediction people perform better when the information is presented in frequency (rather than probability) format as the number of times an event has occurred in a given time period is observable in nature. In contrast, probabilities (in the sense of a number between 0 and 1) are mathematical abstractions.
- Biased response patterns to adaptive problems resulted in lower error costs than unbiased response patterns (error management biases). Cognitive mechanisms can generally produce two types of errors: false positives (taking an action that would have been better not to take), and false negatives (failing to take an action that would have been better to take). An optimal mechanism would make no errors of either type. However, most real world judgment tasks are probabilistic and include an irreducible amount of uncertainty. Hence, some error is likely to occur however good the mechanism. The fitness costs of making each type of error are rarely equal and the brain did not evolve so as to minimize the total error rate. It evolved to minimize the net effect of error on fitness. Where one error is consistently more damaging to fitness than the other, the brain would have evolved to have a bias towards making the less costly error. This is because it is better to make more errors overall if they are relatively cheap in terms of the cost on fitness, survival and reproduction. One example of this is the significant bias towards false positives in assessing cues of disease threat which has far reaching social and societal implications and may lie at the root of many forms of stigmatization and prejudice, including ageism, xenophobia and anti-fat prejudice. Hypersensitivity to disease threat leads to stigmatization or avoidance of individuals who pose no risk of disease transmission whatsoever, yet display cues that were associated with disease threat ancestrally. Members of other cultural groups may be implicitly associated with disease threat as human immune systems are attuned to local diseases. Contact with unfamiliar outgroups might have historically increased the risk of contracting dangerous pathogens unfamiliar to locally-adapted immune systems.

The brain has limited resources and so must be frugal in the models and hence predictions it creates. This is known as the frame problem. The brain confronts an unrelenting risk of combinatorial explosion, in which every detail of every unfolding situation could be explored literally ad infinitum for relevant threats and opportunities. Since this is impossible and the brain cannot neglect the daunting task of producing real-time predictions on all important topics it must use tricks to overcome its limitations. It must also by necessity employ models that contain friction.

The model creation processes is one that has been engineered by evolution to take many, many risks in the interests of timeliness. The emotions, for example, often lead to well adapted, but frequently imperfect behaviours. We:

- helplessly procrastinate when we have important jobs to do
- smoke cigarettes from packages printed with images of lung cancer
- become addicted to liquor and drugs and then watch them destroy our careers, families, and social lives
- cheat on our diets
- cheat on our spouses
- fail to save for the future
- gamble away our hard-earned cash when we know the odds are against us
- have many biases

Though some of these are relatively new environmental challenges, Cheating and procrastination are age-old problems. The above problems are all due to a runaway-but necessary-desire that participates in a heuristic system that chooses behaviors by balancing and time-sharing control of resources between various necessary goals. These are all forms of addiction. Drugs, alcohol, and gambling are all well known as addictive activities. Procrastination is an addiction to laziness-an effective energy-conservative strategy; diet cheating is an addiction to the joys provided by the flavors of sugars and fats; spousal cheating is an addiction to various social and sexual emotions; and wasting your savings may be a result of an addiction to any number of things. The key point here is that each of these behaviors is one that we should be motivated to do-in moderation (except drugs, which hijack our reward systems at a chemical level)-but when the balance is thrown off by improper valuations we behave irrationally.

Choosing how to behave under uncertainty requires a heuristic choice process. Good heuristics give excellent approximations much of the time. But, in the (restricted-by-design) areas where they fail, they give predictably-even pathologically-poor results. The emotions are rational, but the system is a heuristic driver of behavior that operates on incomplete information; so we must accept that the emotions will fail us in some ways, such as overreactions and addictions, that are irresolvable.
